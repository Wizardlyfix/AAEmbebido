{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9ixwracVJsg"
      },
      "source": [
        "# Transfer Learning with MobileNetV2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf1lnhXpVJsf"
      },
      "source": [
        "Base on the Notebook designed by Berkay Alan and Deep Learning AI.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7y9a4mXVJsh"
      },
      "source": [
        "You'll be using transfer learning on a pre-trained CNN to build an image classifier!\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*FIQ3n2znZw4Z9EZJoEQ-JA.jpeg\" style=\"width:300px;height:220px;\">\n",
        "\n",
        "A pre-trained model is a network that's already been trained on a large dataset and saved, which allows you to use it to customize your own model cheaply and efficiently. The one you'll be using, MobileNetV2, was designed to provide fast and computationally efficient performance. It's been pre-trained on ImageNet, a dataset containing over 14 million images and 1000 classes.\n",
        "\n",
        "By the end of this assignment, you will be able to:\n",
        "\n",
        "- Create a dataset from a directory\n",
        "- Preprocess and augment data using the Sequential API\n",
        "- Adapt a pretrained model to new data and train a classifier using the Functional API and MobileNet\n",
        "- Fine-tune a classifier's final layers to improve accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPUeZGA2VJsh"
      },
      "source": [
        "## Table of Content\n",
        "\n",
        "- [1 - Packages](#1)\n",
        "    - [1.1 Create the Dataset and Split it into Training and Validation Sets](#1-1)\n",
        "- [2 - Preprocess and Augment Training Data](#2)\n",
        "    - [Exercise 1 - data_augmenter](#ex-1)\n",
        "- [3 - Using MobileNetV2 for Transfer Learning](#3)\n",
        "    - [3.1 - Inside a MobileNetV2 Convolutional Building Block](#3-1)\n",
        "    - [3.2 - Layer Freezing with the Functional API](#3-2)\n",
        "        - [Exercise 2 - alpaca_model](#ex-2)\n",
        "    - [3.3 - Fine-tuning the Model](#3-3)\n",
        "        - [Exercise 3](#ex-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmAa9GLCVJsi"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0BdNrZfNVJsi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as tfl\n",
        "\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='1-0'></a> Load an Image from your dataset and perform the image cropping and resizing using tensorflow commands, as explained in https://www.tensorflow.org/tutorials/images/data_augmentation. The idea is to simulate the image processing step performed in Edge Impulse platform."
      ],
      "metadata": {
        "id": "0GGe2YDKrRFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO CODE HERE\n"
      ],
      "metadata": {
        "id": "BXoCM3mYsUJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07-4gnmPVJsj"
      },
      "source": [
        "<a name='1-1'></a>\n",
        "### 1.1 Create the Dataset and Split it into Training and Validation Sets\n",
        "\n",
        "When training and evaluating deep learning models in Keras, generating a dataset from image files stored on disk is simple and fast. Call `image_data_set_from_directory()` to read from the directory and create both training and validation datasets.\n",
        "\n",
        "If you're specifying a validation split, you'll also need to specify the subset for each portion. Just set the training set to `subset='training'` and the validation set to `subset='validation'`.\n",
        "\n",
        "You'll also set your seeds to match each other, so your training and validation sets don't overlap.\n",
        "\n",
        "Please upload your dataset into the dataset folder in the content directory, or use the dataset dog and cats employed in the [TensorFLow Tutorial on Transfer Learning](https://www.tensorflow.org/tutorials/images/transfer_learning). Also, let's take a look at the function [image_dataset_from_directory](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsIT7xuHVJsj"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (95, 95)\n",
        "directory = \"dataset/\"\n",
        "train_dataset = image_dataset_from_directory(directory,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             validation_split=0.2,\n",
        "                                             subset='training',\n",
        "                                             seed=42)\n",
        "validation_dataset = image_dataset_from_directory(directory,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             validation_split=0.2,\n",
        "                                             subset='validation',\n",
        "                                             seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBWelrAQVJsk"
      },
      "source": [
        "Now let's take a look at some of the images from the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "014gYsyQVJsl"
      },
      "outputs": [],
      "source": [
        "class_names = train_dataset.class_names\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_dataset.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(class_names[labels[i]])\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRGcaq8DVJsl"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Preprocess and Augment Training Data\n",
        "\n",
        "Using `prefetch()` prevents a memory bottleneck that can occur when reading from disk. It sets aside some data and keeps it ready for when it's needed, by creating a source dataset from your input data, applying a transformation to preprocess it, then iterating over the dataset one element at a time. Because the iteration is streaming, the data doesn't need to fit into memory.\n",
        "\n",
        "You can set the number of elements to prefetch manually, or you can use `tf.data.experimental.AUTOTUNE` to choose the parameters automatically. Autotune prompts `tf.data` to tune that value dynamically at runtime, by tracking the time spent in each operation and feeding those times into an optimization algorithm. The optimization algorithm tries to find the best allocation of its CPU budget across all tunable operations.\n",
        "\n",
        "To increase diversity in the training set and help your model learn the data better, it's standard practice to augment the images by transforming them, i.e., randomly flipping and rotating them. Keras' Sequential API offers a straightforward method for these kinds of data augmentations, with built-in, customizable preprocessing layers. These layers are saved with the rest of your model and can be re-used later.  Ahh, so convenient!\n",
        "\n",
        "As always, you're invited to read the official docs, which you can find for data augmentation [here](https://www.tensorflow.org/tutorials/images/data_augmentation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKvaf8mIVJsl"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTBqx9meVJsm"
      },
      "source": [
        "<a name='ex-1'></a>\n",
        "### Exercise 1 - data_augmenter\n",
        "\n",
        "Implement a function for data augmentation. Use a `Sequential` keras model composed of 2 layers:\n",
        "* `RandomFlip('horizontal')`\n",
        "* `RandomRotation(0.2)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-cd6b3e9f32b1bf37",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "B4HWTH1xVJsm"
      },
      "outputs": [],
      "source": [
        "# UNQ_C1\n",
        "# GRADED FUNCTION: data_augmenter\n",
        "def data_augmenter():\n",
        "    '''\n",
        "    Create a Sequential model composed of 2 layers\n",
        "    Returns:\n",
        "        tf.keras.Sequential\n",
        "    '''\n",
        "    ### START CODE HERE\n",
        "    data_augmentation = tf.keras.Sequential()\n",
        "    data_augmentation.add(RandomFlip(\"horizontal\"))\n",
        "    data_augmentation.add(RandomRotation(0.2))\n",
        "    ### END CODE HERE\n",
        "\n",
        "    return data_augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-f3afa9106c3fad56",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "EJvgHhgoVJsm",
        "outputId": "6b889642-053a-433a-c8c4-be290384c674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92mAll tests passed!\n"
          ]
        }
      ],
      "source": [
        "augmenter = data_augmenter()\n",
        "\n",
        "assert(augmenter.layers[0].name.startswith('random_flip')), \"First layer must be RandomFlip\"\n",
        "assert augmenter.layers[0].mode == 'horizontal', \"RadomFlip parameter must be horizontal\"\n",
        "assert(augmenter.layers[1].name.startswith('random_rotation')), \"Second layer must be RandomRotation\"\n",
        "assert augmenter.layers[1].factor == 0.2, \"Rotation factor must be 0.2\"\n",
        "assert len(augmenter.layers) == 2, \"The model must have only 2 layers\"\n",
        "\n",
        "print('\\033[92mAll tests passed!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evV-dcr-VJsn",
        "outputId": "8373e9fe-3d96-4df8-dfef-2f646370b0ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.layers.preprocessing.image_preprocessing.RandomFlip at 0x7fccbc192ed0>,\n",
              " <tensorflow.python.keras.layers.preprocessing.image_preprocessing.RandomRotation at 0x7fccbc17b290>]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "augmenter.layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXrNTH3BVJsn"
      },
      "source": [
        "Take a look at how an image from the training set has been augmented with simple transformations:\n",
        "\n",
        "From one cute animal, to 9 variations of that cute animal, in three lines of code. Now your model has a lot more to learn from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZsCJHe4VJsn"
      },
      "outputs": [],
      "source": [
        "data_augmentation = data_augmenter()\n",
        "\n",
        "for image, _ in train_dataset.take(1):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    first_image = image[0]\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
        "        plt.imshow(augmented_image[0] / 255)\n",
        "        plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvsSRy9UVJso"
      },
      "source": [
        "Next, you'll apply your first tool from the MobileNet application in TensorFlow, to normalize your input. Since you're using a pre-trained model that was trained on the normalization values [-1,1], it's best practice to reuse that standard with tf.keras.applications.mobilenet_v2.preprocess_input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mqlz-eH6VJso"
      },
      "source": [
        "<font color = 'blue'>\n",
        "\n",
        "**What you should remember:**\n",
        "\n",
        "* When calling image_data_set_from_directory(), specify the train/val subsets and match the seeds to prevent overlap\n",
        "* Use prefetch() to prevent memory bottlenecks when reading from disk\n",
        "* Give your model more to learn from with simple data augmentations like rotation and flipping.\n",
        "* When using a pretrained model, it's best to reuse the weights it was trained on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSrF12OGVJso"
      },
      "outputs": [],
      "source": [
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEaZnM-aVJso"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Using MobileNetV2 for Transfer Learning\n",
        "\n",
        "MobileNetV2 was trained on ImageNet and is optimized to run on mobile and other low-power applications. It's 155 layers deep (just in case you felt the urge to plot the model yourself, prepare for a long journey!) and very efficient for object detection and image segmentation tasks, as well as classification tasks like this one. The architecture has three defining characteristics:\n",
        "\n",
        "*   Depthwise separable convolutions\n",
        "*   Thin input and output bottlenecks between layers\n",
        "*   Shortcut connections between bottleneck layers\n",
        "\n",
        "MobileNetV2 uses depthwise separable convolutions as efficient building blocks. Traditional convolutions are often very resource-intensive, and  depthwise separable convolutions are able to reduce the number of trainable parameters and operations and also speed up convolutions in two steps:\n",
        "\n",
        "1. The first step calculates an intermediate result by convolving on each of the channels independently. This is the depthwise convolution.\n",
        "\n",
        "2. In the second step, another convolution merges the outputs of the previous step into one. This gets a single result from a single feature at a time, and then is applied to all the filters in the output layer. This is the pointwise convolution, or: **Shape of the depthwise convolution X Number of filters.**\n",
        "\n",
        "<img src=\"https://1.bp.blogspot.com/-M8UvZJWNW4E/WsKk-tbzp8I/AAAAAAAAChw/OqxBVPbDygMIQWGug4ZnHNDvuyK5FBMcQCLcBGAs/s1600/image5.png\" style=\"width:650px;height:450px;\">\n",
        "<caption><center> <u> <font color='purple'> <b>Figure 1</b> </u><font color='purple'>  : <b>MobileNetV2 Architecture</b> <br> This diagram was inspired by the original seen <a href=\"https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html#:~:text=MobileNetV2%20is%20a%20significant%20improvement,object%20detection%20and%20semantic%20segmentation.\">here</a>.</center></caption>\n",
        "\n",
        "Each block consists of an inverted residual structure with a bottleneck at each end. These bottlenecks encode the intermediate inputs and outputs in a low dimensional space, and prevent non-linearities from destroying important information.\n",
        "\n",
        "The shortcut connections, which are similar to the ones in traditional residual networks, serve the same purpose of speeding up training and improving predictions. These connections skip over the intermediate convolutions and connect the bottleneck layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpLdjEKvVJso"
      },
      "source": [
        "Let's try to train your base model using all the layers from the pretrained model.\n",
        "\n",
        "Similarly to how you reused the pretrained normalization values MobileNetV2 was trained on, you'll also load the pretrained weights from ImageNet by specifying `weights='imagenet'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Gn9LNtiVJsp",
        "outputId": "b04c68db-f727-4e68-defb-301f08babc5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160.h5\n",
            "14540800/14536120 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "IMG_SHAPE = IMG_SIZE + (3,)\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                               include_top=True,\n",
        "                                               weights='imagenet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV-_kR7VJsp"
      },
      "source": [
        "Print the model summary below to see all the model's layers, the shapes of their outputs, and the total number of parameters, trainable and non-trainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RUH45IxVJsp",
        "outputId": "3a9f1d11-909b-4b99-95d8-864ba8d4f3d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"mobilenetv2_1.00_160\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 160, 160, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Conv1_pad (ZeroPadding2D)       (None, 161, 161, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "Conv1 (Conv2D)                  (None, 80, 80, 32)   864         Conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bn_Conv1 (BatchNormalization)   (None, 80, 80, 32)   128         Conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Conv1_relu (ReLU)               (None, 80, 80, 32)   0           bn_Conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise (Depthw (None, 80, 80, 32)   288         Conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise_BN (Bat (None, 80, 80, 32)   128         expanded_conv_depthwise[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise_relu (R (None, 80, 80, 32)   0           expanded_conv_depthwise_BN[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_project (Conv2D)  (None, 80, 80, 16)   512         expanded_conv_depthwise_relu[0][0\n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_project_BN (Batch (None, 80, 80, 16)   64          expanded_conv_project[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand (Conv2D)         (None, 80, 80, 96)   1536        expanded_conv_project_BN[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand_BN (BatchNormali (None, 80, 80, 96)   384         block_1_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand_relu (ReLU)      (None, 80, 80, 96)   0           block_1_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_1_pad (ZeroPadding2D)     (None, 81, 81, 96)   0           block_1_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise (DepthwiseCon (None, 40, 40, 96)   864         block_1_pad[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise_BN (BatchNorm (None, 40, 40, 96)   384         block_1_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise_relu (ReLU)   (None, 40, 40, 96)   0           block_1_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_1_project (Conv2D)        (None, 40, 40, 24)   2304        block_1_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_1_project_BN (BatchNormal (None, 40, 40, 24)   96          block_1_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand (Conv2D)         (None, 40, 40, 144)  3456        block_1_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand_BN (BatchNormali (None, 40, 40, 144)  576         block_2_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand_relu (ReLU)      (None, 40, 40, 144)  0           block_2_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise (DepthwiseCon (None, 40, 40, 144)  1296        block_2_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise_BN (BatchNorm (None, 40, 40, 144)  576         block_2_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise_relu (ReLU)   (None, 40, 40, 144)  0           block_2_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_2_project (Conv2D)        (None, 40, 40, 24)   3456        block_2_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_2_project_BN (BatchNormal (None, 40, 40, 24)   96          block_2_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_2_add (Add)               (None, 40, 40, 24)   0           block_1_project_BN[0][0]         \n",
            "                                                                 block_2_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand (Conv2D)         (None, 40, 40, 144)  3456        block_2_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand_BN (BatchNormali (None, 40, 40, 144)  576         block_3_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand_relu (ReLU)      (None, 40, 40, 144)  0           block_3_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_3_pad (ZeroPadding2D)     (None, 41, 41, 144)  0           block_3_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise (DepthwiseCon (None, 20, 20, 144)  1296        block_3_pad[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise_BN (BatchNorm (None, 20, 20, 144)  576         block_3_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise_relu (ReLU)   (None, 20, 20, 144)  0           block_3_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_3_project (Conv2D)        (None, 20, 20, 32)   4608        block_3_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_3_project_BN (BatchNormal (None, 20, 20, 32)   128         block_3_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand (Conv2D)         (None, 20, 20, 192)  6144        block_3_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand_BN (BatchNormali (None, 20, 20, 192)  768         block_4_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand_relu (ReLU)      (None, 20, 20, 192)  0           block_4_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise (DepthwiseCon (None, 20, 20, 192)  1728        block_4_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise_BN (BatchNorm (None, 20, 20, 192)  768         block_4_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise_relu (ReLU)   (None, 20, 20, 192)  0           block_4_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_4_project (Conv2D)        (None, 20, 20, 32)   6144        block_4_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_4_project_BN (BatchNormal (None, 20, 20, 32)   128         block_4_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_4_add (Add)               (None, 20, 20, 32)   0           block_3_project_BN[0][0]         \n",
            "                                                                 block_4_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand (Conv2D)         (None, 20, 20, 192)  6144        block_4_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand_BN (BatchNormali (None, 20, 20, 192)  768         block_5_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand_relu (ReLU)      (None, 20, 20, 192)  0           block_5_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise (DepthwiseCon (None, 20, 20, 192)  1728        block_5_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise_BN (BatchNorm (None, 20, 20, 192)  768         block_5_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise_relu (ReLU)   (None, 20, 20, 192)  0           block_5_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_5_project (Conv2D)        (None, 20, 20, 32)   6144        block_5_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_5_project_BN (BatchNormal (None, 20, 20, 32)   128         block_5_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_5_add (Add)               (None, 20, 20, 32)   0           block_4_add[0][0]                \n",
            "                                                                 block_5_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_6_expand (Conv2D)         (None, 20, 20, 192)  6144        block_5_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_6_expand_BN (BatchNormali (None, 20, 20, 192)  768         block_6_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_6_expand_relu (ReLU)      (None, 20, 20, 192)  0           block_6_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_6_pad (ZeroPadding2D)     (None, 21, 21, 192)  0           block_6_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_6_depthwise (DepthwiseCon (None, 10, 10, 192)  1728        block_6_pad[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_6_depthwise_BN (BatchNorm (None, 10, 10, 192)  768         block_6_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_6_depthwise_relu (ReLU)   (None, 10, 10, 192)  0           block_6_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_6_project (Conv2D)        (None, 10, 10, 64)   12288       block_6_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_6_project_BN (BatchNormal (None, 10, 10, 64)   256         block_6_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_7_expand (Conv2D)         (None, 10, 10, 384)  24576       block_6_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_7_expand_BN (BatchNormali (None, 10, 10, 384)  1536        block_7_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_7_expand_relu (ReLU)      (None, 10, 10, 384)  0           block_7_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_7_depthwise (DepthwiseCon (None, 10, 10, 384)  3456        block_7_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_7_depthwise_BN (BatchNorm (None, 10, 10, 384)  1536        block_7_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_7_depthwise_relu (ReLU)   (None, 10, 10, 384)  0           block_7_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_7_project (Conv2D)        (None, 10, 10, 64)   24576       block_7_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_7_project_BN (BatchNormal (None, 10, 10, 64)   256         block_7_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_7_add (Add)               (None, 10, 10, 64)   0           block_6_project_BN[0][0]         \n",
            "                                                                 block_7_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_8_expand (Conv2D)         (None, 10, 10, 384)  24576       block_7_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_8_expand_BN (BatchNormali (None, 10, 10, 384)  1536        block_8_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_8_expand_relu (ReLU)      (None, 10, 10, 384)  0           block_8_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_8_depthwise (DepthwiseCon (None, 10, 10, 384)  3456        block_8_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_8_depthwise_BN (BatchNorm (None, 10, 10, 384)  1536        block_8_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_8_depthwise_relu (ReLU)   (None, 10, 10, 384)  0           block_8_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_8_project (Conv2D)        (None, 10, 10, 64)   24576       block_8_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_8_project_BN (BatchNormal (None, 10, 10, 64)   256         block_8_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_8_add (Add)               (None, 10, 10, 64)   0           block_7_add[0][0]                \n",
            "                                                                 block_8_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_9_expand (Conv2D)         (None, 10, 10, 384)  24576       block_8_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_9_expand_BN (BatchNormali (None, 10, 10, 384)  1536        block_9_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_9_expand_relu (ReLU)      (None, 10, 10, 384)  0           block_9_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_9_depthwise (DepthwiseCon (None, 10, 10, 384)  3456        block_9_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_9_depthwise_BN (BatchNorm (None, 10, 10, 384)  1536        block_9_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_9_depthwise_relu (ReLU)   (None, 10, 10, 384)  0           block_9_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_9_project (Conv2D)        (None, 10, 10, 64)   24576       block_9_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_9_project_BN (BatchNormal (None, 10, 10, 64)   256         block_9_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_9_add (Add)               (None, 10, 10, 64)   0           block_8_add[0][0]                \n",
            "                                                                 block_9_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_10_expand (Conv2D)        (None, 10, 10, 384)  24576       block_9_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_10_expand_BN (BatchNormal (None, 10, 10, 384)  1536        block_10_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_10_expand_relu (ReLU)     (None, 10, 10, 384)  0           block_10_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_10_depthwise (DepthwiseCo (None, 10, 10, 384)  3456        block_10_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_10_depthwise_BN (BatchNor (None, 10, 10, 384)  1536        block_10_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_10_depthwise_relu (ReLU)  (None, 10, 10, 384)  0           block_10_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_10_project (Conv2D)       (None, 10, 10, 96)   36864       block_10_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_10_project_BN (BatchNorma (None, 10, 10, 96)   384         block_10_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_11_expand (Conv2D)        (None, 10, 10, 576)  55296       block_10_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_11_expand_BN (BatchNormal (None, 10, 10, 576)  2304        block_11_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_11_expand_relu (ReLU)     (None, 10, 10, 576)  0           block_11_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_11_depthwise (DepthwiseCo (None, 10, 10, 576)  5184        block_11_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_11_depthwise_BN (BatchNor (None, 10, 10, 576)  2304        block_11_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_11_depthwise_relu (ReLU)  (None, 10, 10, 576)  0           block_11_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_11_project (Conv2D)       (None, 10, 10, 96)   55296       block_11_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_11_project_BN (BatchNorma (None, 10, 10, 96)   384         block_11_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_11_add (Add)              (None, 10, 10, 96)   0           block_10_project_BN[0][0]        \n",
            "                                                                 block_11_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_12_expand (Conv2D)        (None, 10, 10, 576)  55296       block_11_add[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_12_expand_BN (BatchNormal (None, 10, 10, 576)  2304        block_12_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_12_expand_relu (ReLU)     (None, 10, 10, 576)  0           block_12_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_12_depthwise (DepthwiseCo (None, 10, 10, 576)  5184        block_12_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_12_depthwise_BN (BatchNor (None, 10, 10, 576)  2304        block_12_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_12_depthwise_relu (ReLU)  (None, 10, 10, 576)  0           block_12_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_12_project (Conv2D)       (None, 10, 10, 96)   55296       block_12_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_12_project_BN (BatchNorma (None, 10, 10, 96)   384         block_12_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_12_add (Add)              (None, 10, 10, 96)   0           block_11_add[0][0]               \n",
            "                                                                 block_12_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_13_expand (Conv2D)        (None, 10, 10, 576)  55296       block_12_add[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_13_expand_BN (BatchNormal (None, 10, 10, 576)  2304        block_13_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_13_expand_relu (ReLU)     (None, 10, 10, 576)  0           block_13_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_13_pad (ZeroPadding2D)    (None, 11, 11, 576)  0           block_13_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_13_depthwise (DepthwiseCo (None, 5, 5, 576)    5184        block_13_pad[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_13_depthwise_BN (BatchNor (None, 5, 5, 576)    2304        block_13_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_13_depthwise_relu (ReLU)  (None, 5, 5, 576)    0           block_13_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_13_project (Conv2D)       (None, 5, 5, 160)    92160       block_13_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_13_project_BN (BatchNorma (None, 5, 5, 160)    640         block_13_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_14_expand (Conv2D)        (None, 5, 5, 960)    153600      block_13_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_14_expand_BN (BatchNormal (None, 5, 5, 960)    3840        block_14_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_14_expand_relu (ReLU)     (None, 5, 5, 960)    0           block_14_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_14_depthwise (DepthwiseCo (None, 5, 5, 960)    8640        block_14_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_14_depthwise_BN (BatchNor (None, 5, 5, 960)    3840        block_14_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_14_depthwise_relu (ReLU)  (None, 5, 5, 960)    0           block_14_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_14_project (Conv2D)       (None, 5, 5, 160)    153600      block_14_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_14_project_BN (BatchNorma (None, 5, 5, 160)    640         block_14_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_14_add (Add)              (None, 5, 5, 160)    0           block_13_project_BN[0][0]        \n",
            "                                                                 block_14_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_15_expand (Conv2D)        (None, 5, 5, 960)    153600      block_14_add[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_15_expand_BN (BatchNormal (None, 5, 5, 960)    3840        block_15_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_15_expand_relu (ReLU)     (None, 5, 5, 960)    0           block_15_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_15_depthwise (DepthwiseCo (None, 5, 5, 960)    8640        block_15_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_15_depthwise_BN (BatchNor (None, 5, 5, 960)    3840        block_15_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_15_depthwise_relu (ReLU)  (None, 5, 5, 960)    0           block_15_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_15_project (Conv2D)       (None, 5, 5, 160)    153600      block_15_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_15_project_BN (BatchNorma (None, 5, 5, 160)    640         block_15_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_15_add (Add)              (None, 5, 5, 160)    0           block_14_add[0][0]               \n",
            "                                                                 block_15_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_16_expand (Conv2D)        (None, 5, 5, 960)    153600      block_15_add[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_16_expand_BN (BatchNormal (None, 5, 5, 960)    3840        block_16_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_16_expand_relu (ReLU)     (None, 5, 5, 960)    0           block_16_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_16_depthwise (DepthwiseCo (None, 5, 5, 960)    8640        block_16_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_16_depthwise_BN (BatchNor (None, 5, 5, 960)    3840        block_16_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_16_depthwise_relu (ReLU)  (None, 5, 5, 960)    0           block_16_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_16_project (Conv2D)       (None, 5, 5, 320)    307200      block_16_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_16_project_BN (BatchNorma (None, 5, 5, 320)    1280        block_16_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "Conv_1 (Conv2D)                 (None, 5, 5, 1280)   409600      block_16_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "Conv_1_bn (BatchNormalization)  (None, 5, 5, 1280)   5120        Conv_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "out_relu (ReLU)                 (None, 5, 5, 1280)   0           Conv_1_bn[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 1280)         0           out_relu[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Dense)             (None, 1000)         1281000     global_average_pooling2d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 3,538,984\n",
            "Trainable params: 3,504,872\n",
            "Non-trainable params: 34,112\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z1a7tL6VJsp"
      },
      "source": [
        "Note the last 2 layers here. They are the so called top layers, and they are responsible of the classification in the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDLtD3FvVJsq",
        "outputId": "e634ae5d-b44f-4e2b-edac-a6907de05628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "global_average_pooling2d\n",
            "predictions\n"
          ]
        }
      ],
      "source": [
        "nb_layers = len(base_model.layers)\n",
        "print(base_model.layers[nb_layers - 2].name)\n",
        "print(base_model.layers[nb_layers - 1].name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh1bmmbsVJsq"
      },
      "source": [
        "Notice some of the layers in the summary like `Conv2D` and `DepthwiseConv2D` and how they follow the progression of expansion to depthwise convolution to projection. In combination with BatchNormalization and ReLU, these make up the bottleneck layers mentioned earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fv2k5dmVJsq"
      },
      "source": [
        "<font color='blue'>\n",
        "\n",
        "**What you should remember**:\n",
        "\n",
        "* MobileNetV2's unique features are:\n",
        "  * Depthwise separable convolutions that provide lightweight feature filtering and creation\n",
        "  * Input and output bottlenecks that preserve important information on either end of the block\n",
        "* Depthwise separable convolutions deal with both spatial and depth (number of channels) dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZbVk61MVJsq"
      },
      "source": [
        "Next, choose the first batch from the tensorflow dataset to use the images, and run it through the MobileNetV2 base model to test out the predictions on some of your images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAvWHx6hVJsr",
        "outputId": "1b47744a-4d68-42b2-e416-a5423500ed10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 1000)\n"
          ]
        }
      ],
      "source": [
        "image_batch, label_batch = next(iter(train_dataset))\n",
        "feature_batch = base_model(image_batch)\n",
        "print(feature_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H5M-UxgVJsr"
      },
      "outputs": [],
      "source": [
        "#Shows the different label probabilities in one tensor\n",
        "label_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MufAlF5yVJsr"
      },
      "source": [
        "Now decode the predictions made by the model. Earlier, when you printed the shape of the batch, it would have returned (32, 1000). The number 32 refers to the batch size and 1000 refers to the 1000 classes the model was pretrained on. The predictions returned by the base model below follow this format:\n",
        "\n",
        "First the class number, then a human-readable label, and last the probability of the image belonging to that class. You'll notice that there are two of these returned for each image in the batch - these the top two probabilities returned for that image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZQXoRkAVJsr",
        "outputId": "14086e14-f6fa-4dff-bc58-ad7de3d98f0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "40960/35363 [==================================] - 0s 0us/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[('n04589890', 'window_screen', 0.42582113),\n",
              "  ('n02708093', 'analog_clock', 0.09275588)],\n",
              " [('n04589890', 'window_screen', 0.23985851),\n",
              "  ('n03887697', 'paper_towel', 0.14802593)],\n",
              " [('n04589890', 'window_screen', 0.7449468),\n",
              "  ('n03598930', 'jigsaw_puzzle', 0.021948555)],\n",
              " [('n04589890', 'window_screen', 0.3354602),\n",
              "  ('n03530642', 'honeycomb', 0.0762895)],\n",
              " [('n04589890', 'window_screen', 0.27327338),\n",
              "  ('n03733281', 'maze', 0.08846959)],\n",
              " [('n04589890', 'window_screen', 0.6745217),\n",
              "  ('n03530642', 'honeycomb', 0.076599255)],\n",
              " [('n04589890', 'window_screen', 0.79128474),\n",
              "  ('n04209239', 'shower_curtain', 0.0924166)],\n",
              " [('n04589890', 'window_screen', 0.1646301),\n",
              "  ('n03598930', 'jigsaw_puzzle', 0.08749153)],\n",
              " [('n03598930', 'jigsaw_puzzle', 0.37021384),\n",
              "  ('n04589890', 'window_screen', 0.099569514)],\n",
              " [('n04589890', 'window_screen', 0.6161648),\n",
              "  ('n03887697', 'paper_towel', 0.054877095)],\n",
              " [('n03530642', 'honeycomb', 0.254489),\n",
              "  ('n04589890', 'window_screen', 0.24874294)],\n",
              " [('n04589890', 'window_screen', 0.8050193),\n",
              "  ('n04590129', 'window_shade', 0.038429063)],\n",
              " [('n04589890', 'window_screen', 0.1981784),\n",
              "  ('n03388043', 'fountain', 0.10614327)],\n",
              " [('n04589890', 'window_screen', 0.19500774),\n",
              "  ('n02128385', 'leopard', 0.09882233)],\n",
              " [('n04589890', 'window_screen', 0.9158902),\n",
              "  ('n04332243', 'strainer', 0.006900199)],\n",
              " [('n04589890', 'window_screen', 0.36936575),\n",
              "  ('n03598930', 'jigsaw_puzzle', 0.26770484)],\n",
              " [('n01675722', 'banded_gecko', 0.51328546), ('n02219486', 'ant', 0.15775205)],\n",
              " [('n03729826', 'matchstick', 0.0832222), ('n03733281', 'maze', 0.07249375)],\n",
              " [('n03291819', 'envelope', 0.21836512), ('n06359193', 'web_site', 0.1876467)],\n",
              " [('n04589890', 'window_screen', 0.3265771),\n",
              "  ('n03530642', 'honeycomb', 0.13020417)],\n",
              " [('n04589890', 'window_screen', 0.8482419),\n",
              "  ('n03598930', 'jigsaw_puzzle', 0.022730079)],\n",
              " [('n04589890', 'window_screen', 0.9029828),\n",
              "  ('n04209239', 'shower_curtain', 0.011390656)],\n",
              " [('n03347037', 'fire_screen', 0.41475636),\n",
              "  ('n04589890', 'window_screen', 0.372897)],\n",
              " [('n04589890', 'window_screen', 0.5712924),\n",
              "  ('n04404412', 'television', 0.14525095)],\n",
              " [('n04589890', 'window_screen', 0.3606223),\n",
              "  ('n03530642', 'honeycomb', 0.13481905)],\n",
              " [('n04589890', 'window_screen', 0.23788457),\n",
              "  ('n03598930', 'jigsaw_puzzle', 0.13157865)],\n",
              " [('n04589890', 'window_screen', 0.25694132),\n",
              "  ('n03598930', 'jigsaw_puzzle', 0.16737717)],\n",
              " [('n06359193', 'web_site', 0.13842636),\n",
              "  ('n04589890', 'window_screen', 0.12990728)],\n",
              " [('n04589890', 'window_screen', 0.32301164),\n",
              "  ('n03598930', 'jigsaw_puzzle', 0.081905656)],\n",
              " [('n03598930', 'jigsaw_puzzle', 0.65311396),\n",
              "  ('n04589890', 'window_screen', 0.067997806)],\n",
              " [('n04589890', 'window_screen', 0.74575704),\n",
              "  ('n03347037', 'fire_screen', 0.08067248)],\n",
              " [('n03733281', 'maze', 0.08606168), ('n06359193', 'web_site', 0.08539752)]]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model.trainable = False\n",
        "image_var = tf.Variable(image_batch)\n",
        "pred = base_model(image_var)\n",
        "\n",
        "tf.keras.applications.mobilenet_v2.decode_predictions(pred.numpy(), top=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VpRz6SWVJss"
      },
      "source": [
        "Uh-oh. There's a whole lot of labels here, some of them hilariously wrong, but none of them say \"alpaca.\"\n",
        "\n",
        "This is because MobileNet pretrained over ImageNet doesn't have the correct labels for alpacas, so when you use the full model, all you get is a bunch of incorrectly classified images.\n",
        "\n",
        "Fortunately, you can delete the top layer, which contains all the classification labels, and create a new classification layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM6lKVGvVJss"
      },
      "source": [
        "<a name='3-2'></a>\n",
        "### 3.2 - Layer Freezing with the Functional API\n",
        "\n",
        "In the next sections, you'll see how you can use a pretrained model to modify the classifier task so that it's able to recognize alpacas. You can achieve this in three steps:\n",
        "\n",
        "1. Delete the top layer (the classification layer)\n",
        "    * Set `include_top` in `base_model` as False\n",
        "2. Add a new classifier layer\n",
        "    * Train only one layer by freezing the rest of the network\n",
        "    * As mentioned before, a single neuron is enough to solve a binary classification problem.\n",
        "3. Freeze the base model and train the newly-created classifier layer\n",
        "    * Set `base model.trainable=False` to avoid changing the weights and train *only* the new layer\n",
        "    * Set training in `base_model` to False to avoid keeping track of statistics in the batch norm layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zTMOu1CVJss"
      },
      "source": [
        "<a name='ex-2'></a>\n",
        "### Exercise 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-106ac76f39286ee3",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "mblrGDuaVJss"
      },
      "outputs": [],
      "source": [
        "def alpaca_model(image_shape=IMG_SIZE, data_augmentation=data_augmenter()):\n",
        "    ''' Define a tf.keras model for binary classification out of the MobileNetV2 model\n",
        "    Arguments:\n",
        "        image_shape -- Image width and height\n",
        "        data_augmentation -- data augmentation function\n",
        "    Returns:\n",
        "    Returns:\n",
        "        tf.keras.model\n",
        "    '''\n",
        "\n",
        "\n",
        "    input_shape = image_shape + (3,)\n",
        "\n",
        "    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape,\n",
        "                                                   include_top=False, # <== Important!!!!\n",
        "                                                   weights='imagenet') # From imageNet\n",
        "\n",
        "    # freeze the base model by making it non trainable\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # create the input layer (Same as the imageNetv2 input size)\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # apply data augmentation to the inputs\n",
        "    x = data_augmentation(inputs)\n",
        "\n",
        "    # data preprocessing using the same weights the model was trained on\n",
        "    x = preprocess_input(x)\n",
        "\n",
        "    # set training to False to avoid keeping track of statistics in the batch norm layer\n",
        "    x = base_model(x, training=False)\n",
        "\n",
        "    # add the new Binary classification layers\n",
        "    # use global avg pooling to summarize the info in each channel\n",
        "    x = tfl.GlobalAveragePooling2D()(x)\n",
        "    #include dropout with probability of 0.2 to avoid overfitting\n",
        "    x = tfl.Dropout(0.2)(x)\n",
        "\n",
        "    # create a prediction layer with one neuron (as a classifier only needs one)\n",
        "    prediction_layer = tfl.Dense(1)\n",
        "\n",
        "    outputs = prediction_layer(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqQ1MbQTVJsy"
      },
      "source": [
        "Create your new model using the data_augmentation function defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgDRUfrPVJsy"
      },
      "outputs": [],
      "source": [
        "model2 = alpaca_model(IMG_SIZE, data_augmentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pEEDDREVJsz"
      },
      "source": [
        "The base learning rate has been set for you, so you can go ahead and compile the new model and run it for 5 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zP7DFA6VJsz"
      },
      "outputs": [],
      "source": [
        "base_learning_rate = 0.001\n",
        "model2.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJE0YAv-VJsz",
        "outputId": "8734fba4-6480-4f77-e361-2b6008e5128a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "9/9 [==============================] - 9s 1s/step - loss: 0.6763 - accuracy: 0.5878 - val_loss: 0.5119 - val_accuracy: 0.6000\n",
            "Epoch 2/5\n",
            "9/9 [==============================] - 8s 857ms/step - loss: 0.5481 - accuracy: 0.6985 - val_loss: 0.4036 - val_accuracy: 0.8154\n",
            "Epoch 3/5\n",
            "9/9 [==============================] - 8s 855ms/step - loss: 0.4993 - accuracy: 0.7519 - val_loss: 0.3671 - val_accuracy: 0.7538\n",
            "Epoch 4/5\n",
            "9/9 [==============================] - 8s 844ms/step - loss: 0.4304 - accuracy: 0.7328 - val_loss: 0.3704 - val_accuracy: 0.6769\n",
            "Epoch 5/5\n",
            "9/9 [==============================] - 8s 843ms/step - loss: 0.4159 - accuracy: 0.7710 - val_loss: 0.2857 - val_accuracy: 0.8769\n"
          ]
        }
      ],
      "source": [
        "initial_epochs = 5\n",
        "history = model2.fit(train_dataset, validation_data=validation_dataset, epochs=initial_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPuhbuvpVJsz"
      },
      "source": [
        "Plot the training and validation accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM0rpyR6VJs0"
      },
      "outputs": [],
      "source": [
        "acc = [0.] + history.history['accuracy']\n",
        "val_acc = [0.] + history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0gZSwQvVJs0",
        "outputId": "95129d14-f895-4ecf-b3a7-e85549ae0031"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['alpaca', 'not alpaca']"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Wgbre0VJs0"
      },
      "source": [
        "The results are ok, but could be better. Next, try some fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv6NghlTVJs3"
      },
      "source": [
        "<font color='blue'>\n",
        "\n",
        "**What you should remember**:\n",
        "\n",
        "* To adapt the classifier to new data: Delete the top layer, add a new classification layer, and train only on that layer\n",
        "* When freezing layers, avoid keeping track of statistics (like in the batch normalization layer)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}